"""
Yerel LLM Ã‡Ä±karÄ±mÄ± iÃ§in Ollama Ä°stemcisi
RTX 3050 4GB VRAM iÃ§in optimize edilmiÅŸtir
"""

import requests
import json
import config
from utils.logger import setup_logger

logger = setup_logger("OllamaClient")


class OllamaClient:
    """Ollama API Ä°stemcisi"""
    
    def __init__(self, model_name=None, host=None):
        """
        ArgÃ¼manlar:
            model_name: KullanÄ±lacak model (varsayÄ±lanÄ± config'den alÄ±r)
            host: Ollama ana bilgisayar URL'si (varsayÄ±lan localhost)
        """
        self.model_name = model_name or config.LLM_MODEL
        self.host = host or "http://127.0.0.1:11434"
        self.api_url = f"{self.host}/api/generate"
        
        # Ollama'nÄ±n Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± kontrol et
        self.check_connection()
    
    def check_connection(self):
        """Ollama sunucusunun Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± doÄŸrula"""
        try:
            response = requests.get(f"{self.host}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                model_names = [m["name"] for m in models]
                
                if self.model_name in model_names:
                    logger.info(f"âœ… Ollama baÄŸlandÄ±, '{self.model_name}' modeli mevcut")
                else:
                    logger.warning(f"âš ï¸ '{self.model_name}' modeli bulunamadÄ±. Mevcut modeller: {model_names}")
                    logger.warning(f"   Ã‡alÄ±ÅŸtÄ±rÄ±n: ollama pull {self.model_name}")
            else:
                logger.error("âŒ Ollama sunucusu yanÄ±t vermiyor")
        
        except requests.RequestException as e:
            logger.error(f"âŒ {self.host} adresindeki Ollama'ya baÄŸlanÄ±lamÄ±yor")
            logger.error(f"   Ollama'nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun: ollama serve")
            logger.error(f"   Hata: {str(e)}")
    
    def generate(self, prompt, system_prompt=None, temperature=None, max_tokens=None):
        """
        Ollama'dan yanÄ±t Ã¼ret
        
        ArgÃ¼manlar:
            prompt: KullanÄ±cÄ± komutu
            system_prompt: Sistem komutu
            temperature: Ã–rnekleme sÄ±caklÄ±ÄŸÄ± (varsayÄ±lanÄ± config'den alÄ±r)
            max_tokens: Ãœretilecek maksimum token sayÄ±sÄ± (varsayÄ±lanÄ± config'den alÄ±r)
            
        DÃ¶ner:
            Ãœretilen metin yanÄ±tÄ±
        """
        if temperature is None:
            temperature = config.LLM_TEMPERATURE
        
        if max_tokens is None:
            max_tokens = config.LLM_MAX_TOKENS
        
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
                "top_p": getattr(config, "LLM_TOP_P", 0.1),
                "num_ctx": getattr(config, "LLM_CONTEXT_WINDOW", 2048)
            }
        }
        
        if system_prompt:
            payload["system"] = system_prompt
        
        try:
            logger.debug(f"ğŸ¤– Ollama'ya ({self.model_name}) istek gÃ¶nderiliyor...")
            
            response = requests.post(self.api_url, json=payload, timeout=180)  # 4GB GPU'lar iÃ§in 180s yapÄ±ldÄ±
            
            if response.status_code == 200:
                result = response.json()
                generated_text = result.get("response", "")
                
                logger.debug(f"âœ… LLM yanÄ±tÄ± alÄ±ndÄ± ({len(generated_text)} karakter)")
                
                return generated_text
            else:
                logger.error(f"âŒ Ollama API hatasÄ±: {response.status_code}")
                return None
        
        except requests.Timeout:
            logger.error("âŒ Ollama isteÄŸi zaman aÅŸÄ±mÄ±na uÄŸradÄ±")
            return None
        
        except Exception as e:
            logger.error(f"âŒ Ollama Ã¼retimi baÅŸarÄ±sÄ±z oldu: {str(e)}")
            return None
    
    def generate_json(self, prompt, system_prompt=None):
        """
        JSON yanÄ±tÄ± Ã¼ret (format zorlamasÄ± ile)
        
        ArgÃ¼manlar:
            prompt: KullanÄ±cÄ± komutu
            system_prompt: Sistem komutu
            
        DÃ¶ner:
            AyrÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ JSON sÃ¶zlÃ¼ÄŸÃ¼ veya baÅŸarÄ±sÄ±z olursa None
        """
        # Komuta JSON formatÄ± talimatÄ±nÄ± ekle
        json_instruction = "\nSADECE geÃ§erli JSON ile yanÄ±t ver. Markdown formatÄ± veya aÃ§Ä±klama ekleme."
        full_prompt = prompt + json_instruction
        
        response_text = self.generate(full_prompt, system_prompt, temperature=0.1)
        
        if not response_text:
            return None
        
        # JSON ayrÄ±ÅŸtÄ±rmayÄ± dene
        try:
            # Varsa markdown kod bloklarÄ±nÄ± kaldÄ±r
            cleaned = response_text.strip()
            if cleaned.startswith("```json"):
                cleaned = cleaned[7:]
            if cleaned.startswith("```"):
                cleaned = cleaned[3:]
            if cleaned.endswith("```"):
                cleaned = cleaned[:-3]
            
            cleaned = cleaned.strip()
            
            return json.loads(cleaned)
        
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON yanÄ±tÄ± ayrÄ±ÅŸtÄ±rÄ±lamadÄ±: {str(e)}")
            logger.debug(f"Ham yanÄ±t: {response_text[:200]}...")
            return None
